import requests
import json
import sys
import os
import time
import logging
import math
import argparse
import csv
import copy
from queue import Queue, Empty
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps

from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- 0. ÏÑ§Ï†ï Î∞è Ïú†Ìã∏Î¶¨Ìã∞ ---

@dataclass(frozen=True)
class Config:
    """Ïä§ÌÅ¨Î¶ΩÌä∏ ÏÑ§Ï†ï Í∞íÏùÑ Í¥ÄÎ¶¨"""
    kakao_api_key: str
    chromedriver_path: str  # chromedriver Í≤ΩÎ°úÎ•º ÏßÅÏ†ë Î∞õÎèÑÎ°ù ÏÑ§Ï†ï
    max_workers: int
    category_map: Dict[str, List[str]]
    max_crawl_count: int = 10
    scroll_attempts: int = 10
    max_reviews_per_star: int = 3

def setup_logging():
    """Ïä§ÌÅ¨Î¶ΩÌä∏ Ï†ÑÏó≠ Î°úÍπÖ ÏãúÏä§ÌÖú ÏÑ§Ï†ï"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

def load_config(path: str = "config.json") -> Config:
    """ÏÑ§Ï†ï ÌååÏùºÏùÑ Î°úÎìú"""
    if not os.path.exists(path):
        logging.critical(f"'{path}' ÏÑ§Ï†ï ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. ÏòàÏãú ÌååÏùºÏùÑ Ï∞∏Í≥†ÌïòÏó¨ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.")
        sys.exit(1)
    with open(path, 'r', encoding='utf-8') as f:
        config_data = json.load(f)
    return Config(**config_data)

def retry(attempts: int = 3, delay: float = 1.0):
    """ÏòàÏô∏ Î∞úÏÉù Ïãú ÏûëÏóÖÏùÑ Ïû¨ÏãúÎèÑÌïòÎäî Îç∞ÏΩîÎ†àÏù¥ÌÑ∞"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logging.warning(f"'{func.__name__}' ÏûëÏóÖ Ïã§Ìå® (ÏãúÎèÑ {attempt + 1}/{attempts}). {delay}Ï¥à ÌõÑ Ïû¨ÏãúÎèÑ. Ïò§Î•ò: {e}")
                    time.sleep(delay)
            raise last_exception
        return wrapper
    return decorator

# --- 1. ÌÅ¨Î°§Îü¨ ÌÅ¥ÎûòÏä§ Ï†ïÏùò ---

class KakaoMapCrawler:
    """Ïπ¥Ïπ¥Ïò§Îßµ ÌÅ¨Î°§Îü¨ (ÏïàÏ†ïÌôîÎêú ÎìúÎùºÏù¥Î≤Ñ ÌíÄ Î∞è Ìö®Ïú®Ï†ÅÏù∏ Ïä§ÌÅ¨Î°§)"""

    DISCOVERY_API_URL = "https://dapi.kakao.com/v2/local/search/keyword.json"

    SELECTORS = {
        "rating": "span.num_star",
        "menu_tab": 'a.link_tab[href="#menuInfo"]',
        "menu_wrap": "div.wrap_goods",
        "menu_list": 'ul.list_goods > li',
        "menu_name": 'strong.tit_item',
        "menu_price": 'p.desc_item',
        "review_tab": 'a.link_tab[href="#comment"]',
        "review_list_wrap": "ul.list_review",
        "review_item": "ul.list_review > li",
        "review_rating": "span.figure_star.on",
        "review_content": "p.desc_review",
    }

    def __init__(self, config: Config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"KakaoAK {self.config.kakao_api_key}"})
        self.driver_pool = Queue(maxsize=self.config.max_workers)
        self._initialize_driver_pool()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def _create_driver(self) -> WebDriver:
        """ÏÉàÎ°úÏö¥ Selenium WebDriver Ïù∏Ïä§ÌÑ¥Ïä§Î•º ÏÉùÏÑ±"""
        try:
            options = Options()
            options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})
            options.add_argument("--headless"); options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage"); options.add_argument("--log-level=3")
            # ÏÑ§Ï†ï ÌååÏùºÏùò Í≤ΩÎ°úÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÎìúÎùºÏù¥Î≤Ñ ÏÑúÎπÑÏä§ ÏÑ§Ï†ï
            service = Service(executable_path=self.config.chromedriver_path)
            driver = webdriver.Chrome(service=service, options=options)
            return driver
        except Exception as e:
            logging.error(f"ÎìúÎùºÏù¥Î≤Ñ ÏÉùÏÑ± Ïã§Ìå®: {e}")
            raise

    def _initialize_driver_pool(self):
        """ÎìúÎùºÏù¥Î≤Ñ ÌíÄÏùÑ Ï¥àÍ∏∞Ìôî"""
        logging.info(f"Selenium ÎìúÎùºÏù¥Î≤Ñ ÌíÄ Ï¥àÍ∏∞Ìôî Ï§ë... (ÏõåÏª§ Ïàò: {self.config.max_workers})")
        for _ in range(self.config.max_workers):
            try:
                driver = self._create_driver()
                self.driver_pool.put(driver)
            except Exception:
                continue
        if self.driver_pool.empty():
            logging.critical("ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÎìúÎùºÏù¥Î≤ÑÍ∞Ä ÏóÜÏäµÎãàÎã§. Chromedriver Í≤ΩÎ°úÎ•º ÌôïÏù∏ÌïòÏÑ∏Ïöî. Ïä§ÌÅ¨Î¶ΩÌä∏Î•º Ï¢ÖÎ£åÌï©ÎãàÎã§.")
            sys.exit(1)
        logging.info(f"Ï¥ù {self.driver_pool.qsize()}Í∞úÏùò ÎìúÎùºÏù¥Î≤ÑÍ∞Ä ÏÑ±Í≥µÏ†ÅÏúºÎ°ú ÏÉùÏÑ±ÎêòÏóàÏäµÎãàÎã§.")

    def discover_restaurants(self, lon: str, lat: str, radius: int, target_count: int) -> List[Dict[str, Any]]:
        """ÏúÑÏπò Í∏∞Î∞ò ÎßõÏßë ÌÉêÏÉâ"""
        center_lon, center_lat = float(lon), float(lat)
        logging.info(f"üó∫Ô∏è  Ï¢åÌëú (Í≤ΩÎèÑ:{lon}, ÏúÑÎèÑ:{lat}) Î∞òÍ≤Ω {radius}m ÎÇ¥ÏóêÏÑú ÏµúÎåÄ {target_count}Í∞ú ÏùåÏãùÏ†ê Í≤ÄÏÉâ ÏãúÏûë...")

        unique_restaurants = {}
        lat_offset = (radius / 2) / 111000
        lon_offset = (radius / 2) / (111000 * math.cos(math.radians(center_lat)))
        search_points = [(str(center_lon + j * lon_offset), str(center_lat + i * lat_offset)) for i in range(-1, 2) for j in range(-1, 2)]

        for p_lon, p_lat in search_points:
            if len(unique_restaurants) >= target_count:
                logging.info(f"Î™©Ìëú Í∞úÏàò {target_count}Í∞úÎ•º Ï¥àÍ≥ºÌïòÏó¨ ÌÉêÏÉâÏùÑ Ï§ëÎã®Ìï©ÎãàÎã§.")
                break
            for page in range(1, 4):
                params = {"query": "ÎßõÏßë", "page": page, "size": 15, "category_group_code": "FD6",
                          'x': p_lon, 'y': p_lat, 'radius': radius, 'sort': 'distance'}
                try:
                    response = self.session.get(self.DISCOVERY_API_URL, params=params, timeout=5)
                    response.raise_for_status()
                    data = response.json()
                    if not data.get('documents'): break
                    for doc in data['documents']:
                        unique_restaurants[doc['id']] = doc
                    if len(unique_restaurants) >= target_count or data['meta']['is_end']: break
                except requests.RequestException as e:
                    logging.error(f"API ÏöîÏ≤≠ Ïò§Î•ò (Lon: {p_lon}, Lat: {p_lat}, Page: {page}): {e}")
                    break

        restaurant_list = list(unique_restaurants.values())
        logging.info(f"‚úÖ Ï¥ù {len(search_points)}Í∞ú ÏßÄÏ†ê ÌÉêÏÉâ ÏôÑÎ£å. {len(restaurant_list)}Í∞úÏùò Í≥†Ïú† ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ ÌôïÏ†ï.")
        return restaurant_list

    def _scrape_rating(self, wait: WebDriverWait) -> Optional[str]:
        try:
            rating_tag = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["rating"])))
            return rating_tag.text
        except TimeoutException:
            return None

    def _scrape_menus(self, driver: WebDriver, wait: WebDriverWait) -> List[Dict[str, str]]:
        menus = []
        try:
            menu_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, self.SELECTORS["menu_tab"])))
            menu_tab.click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["menu_wrap"])))

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            for item in soup.select(self.SELECTORS["menu_list"]):
                name_tag = item.select_one(self.SELECTORS["menu_name"])
                if not name_tag or "ÎçîÎ≥¥Í∏∞" in name_tag.get_text(): continue

                price_tag = item.select_one(self.SELECTORS["menu_price"])
                menus.append({
                    "name": name_tag.get_text(strip=True),
                    "price": price_tag.get_text(strip=True) if price_tag else "Í∞ÄÍ≤© Ï†ïÎ≥¥ ÏóÜÏùå"
                })
        except TimeoutException:
            pass
        return menus

    def _scrape_reviews(self, driver: WebDriver, wait: WebDriverWait) -> Dict[int, List[str]]:
        """Î¶¨Î∑∞ Ïä§ÌÅ¨Î°§ Î∞è ÌååÏã± Ìö®Ïú® Í∞úÏÑ†"""
        reviews = {star: [] for star in range(1, 6)}
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, self.SELECTORS["review_tab"])))
            review_tab.click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["review_list_wrap"])))

            last_review_count = 0
            for _ in range(self.config.scroll_attempts):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(0.8)

                current_reviews = driver.find_elements(By.CSS_SELECTOR, self.SELECTORS["review_item"])
                if len(current_reviews) == last_review_count:
                    break
                last_review_count = len(current_reviews)

            all_review_elements = driver.find_elements(By.CSS_SELECTOR, self.SELECTORS["review_item"])
            for review_item_el in all_review_elements:
                soup = BeautifulSoup(review_item_el.get_attribute('outerHTML'), 'html.parser')
                rating = len(soup.select(self.SELECTORS["review_rating"]))
                content_tag = soup.select_one(self.SELECTORS["review_content"])
                content = content_tag.get_text(strip=True) if content_tag else ""

                if rating in reviews and len(reviews[rating]) < self.config.max_reviews_per_star:
                    reviews[rating].append(content)
        except TimeoutException:
            pass
        return reviews

    def _scrape_page_data(self, driver: WebDriver, place_id: str) -> Dict[str, Any]:
        base_url = f"https://place.map.kakao.com/{place_id}"
        data = {"overall_rating": None, "menus": [], "reviews": {}}

        driver.get(base_url)
        wait = WebDriverWait(driver, 10)

        data["overall_rating"] = self._scrape_rating(wait)
        data["menus"] = self._scrape_menus(driver, wait)
        data["reviews"] = self._scrape_reviews(driver, wait)

        return data

    def _simplify_category(self, category_name: Optional[str]) -> str:
        """ÏÑ§Ï†ï ÌååÏùºÏùò Ïπ¥ÌÖåÍ≥†Î¶¨ ÎßµÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î∂ÑÎ•ò"""
        if not category_name:
            return "Í∏∞ÌÉÄ"
        for simple_cat, keywords in self.config.category_map.items():
            if any(keyword in category_name for keyword in keywords):
                return simple_cat
        return "Í∏∞ÌÉÄ"

    @retry(attempts=2, delay=2)
    def _process_restaurant(self, restaurant_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Í∞úÎ≥Ñ Î†àÏä§ÌÜ†Îûë Ï≤òÎ¶¨ (ÏïàÏ†ïÏ†ÅÏù∏ ÎìúÎùºÏù¥Î≤Ñ ÍµêÏ≤¥ Î°úÏßÅ Ìè¨Ìï®)"""
        place_name, place_id = restaurant_info.get('place_name'), restaurant_info.get('id')
        if not all([place_name, place_id]): return None

        driver = None
        driver_is_faulty = False
        try:
            driver = self.driver_pool.get(timeout=20)
            scraped_data = self._scrape_page_data(driver, place_id)

            detailed_category = restaurant_info.get('category_name')
            simple_category = self._simplify_category(detailed_category)

            return {
                "restaurant_name": place_name, "category": simple_category,
                "overall_rating": scraped_data["overall_rating"],
                "phone_number": restaurant_info.get('phone'),
                "address": restaurant_info.get('road_address_name'),
                "latitude": restaurant_info.get('y'), "longitude": restaurant_info.get('x'),
                "kakao_map_url": restaurant_info.get('place_url'),
                "menu_items": scraped_data["menus"], "reviews_by_star": scraped_data["reviews"],
                "crawled_at": datetime.now().isoformat()
            }
        except WebDriverException as e:
            driver_is_faulty = True
            logging.error(f"'{place_name}' Ï≤òÎ¶¨ Ï§ë ÎìúÎùºÏù¥Î≤Ñ Ïò§Î•ò Î∞úÏÉù: {e.msg[:100]}")
            raise e
        except Empty:
            logging.error("ÎìúÎùºÏù¥Î≤Ñ ÌíÄÏù¥ ÎπÑÏñ¥ÏûàÏñ¥ ÏûëÏóÖÏùÑ Ï≤òÎ¶¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
            return None
        except Exception as e:
            logging.error(f"'{place_name}'({place_id}) Ï≤òÎ¶¨ Ï§ë ÏòàÏô∏ Î∞úÏÉù: {e}", exc_info=False)
            raise e
        finally:
            if driver:
                if driver_is_faulty:
                    logging.warning(f"Í≤∞Ìï®Ïù¥ Î∞úÏÉùÌïú ÎìúÎùºÏù¥Î≤ÑÎ•º Ï¢ÖÎ£åÌïòÍ≥† ÏÉà ÎìúÎùºÏù¥Î≤ÑÎ°ú ÍµêÏ≤¥Ìï©ÎãàÎã§.")
                    try:
                        driver.quit()
                        new_driver = self._create_driver()
                        self.driver_pool.put(new_driver)
                    except Exception as create_err:
                        logging.error(f"ÏÉà ÎìúÎùºÏù¥Î≤Ñ ÏÉùÏÑ±/Ï∂îÍ∞Ä Ïã§Ìå®: {create_err}")
                else:
                    self.driver_pool.put(driver)

    def run(self, lon: str, lat: str, radius: int, max_count: int):
        restaurant_list = self.discover_restaurants(lon, lat, radius, target_count=max_count)
        if not restaurant_list: return

        restaurant_list_to_crawl = restaurant_list[:max_count]
        logging.info(f"ÌÉêÏÉâÎêú {len(restaurant_list)}Í∞ú Ï§ë, {len(restaurant_list_to_crawl)}Í∞úÏóê ÎåÄÌïú ÌÅ¨Î°§ÎßÅÏùÑ ÏãúÏûëÌï©ÎãàÎã§.")

        all_results = []
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            future_to_info = {executor.submit(self._process_restaurant, info): info for info in restaurant_list_to_crawl}
            for future in tqdm(as_completed(future_to_info), total=len(restaurant_list_to_crawl), desc="ÌÅ¨Î°§ÎßÅ ÏßÑÌñâÎ•†"):
                try:
                    result = future.result()
                    if result: all_results.append(result)
                except Exception as e:
                    info = future_to_info[future]
                    logging.error(f"'{info.get('place_name')}' ÏµúÏ¢Ö Ï≤òÎ¶¨ Ïã§Ìå®: {e.__class__.__name__}")

        if all_results:
            self.save_results(all_results)
        else:
            logging.warning("Ïú†Ìö®Ìïú ÌÅ¨Î°§ÎßÅ Í≤∞Í≥ºÍ∞Ä ÏóÜÏñ¥ ÌååÏùºÏùÑ Ï†ÄÏû•ÌïòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")

        logging.info("üéâ Î™®Îì† ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.")

    def save_results(self, results: List[Dict[str, Any]]):
        output_dir = "json_output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = os.path.join(output_dir, f"search_results_{timestamp}")

        # JSON Ï†ÄÏû•
        json_filename = f"{base_filename}.json"
        with open(json_filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        logging.info(f"‚úÖ ÏÑ±Í≥µ! {len(results)}Í∞ú ÏùåÏãùÏ†ê Ï†ïÎ≥¥Î•º '{json_filename}'Ïóê Ï†ÄÏû•ÌñàÏäµÎãàÎã§.")

        # CSV Ï†ÄÏû•
        if not results: return
        csv_filename = f"{base_filename}.csv"
        results_for_csv = copy.deepcopy(results)
        for item in results_for_csv:
            item['menu_items'] = json.dumps(item['menu_items'], ensure_ascii=False)
            item['reviews_by_star'] = json.dumps(item['reviews_by_star'], ensure_ascii=False)

        fieldnames = results_for_csv[0].keys()
        with open(csv_filename, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results_for_csv)
        logging.info(f"‚úÖ ÏÑ±Í≥µ! ÎèôÏùºÌïú Ï†ïÎ≥¥Î•º '{csv_filename}'ÏóêÎèÑ Ï†ÄÏû•ÌñàÏäµÎãàÎã§.")

    def close(self):
        logging.info("Î™®Îì† Selenium ÎìúÎùºÏù¥Î≤ÑÎ•º Ï¢ÖÎ£åÌï©ÎãàÎã§...")
        while not self.driver_pool.empty():
            try:
                driver = self.driver_pool.get_nowait()
                driver.quit()
            except Empty: break
        self.session.close()
        logging.info("Ï†ïÎ¶¨ ÏôÑÎ£å.")

# --- 2. Î©îÏù∏ Ïã§Ìñâ Î°úÏßÅ ---
def main():
    setup_logging()

    parser = argparse.ArgumentParser(description="Ïπ¥Ïπ¥Ïò§Îßµ ÎßõÏßë Ï†ïÎ≥¥ ÌÅ¨Î°§Îü¨ (ÏàòÎèô Í≤ΩÎ°ú ÏÑ§Ï†ï)")
    parser.add_argument("--lon", type=str, default="126.9648", help="Ï§ëÏã¨ Í≤ΩÎèÑ (Ïòà: 126.9648 for Ïö©ÏÇ∞Ïó≠)")
    parser.add_argument("--lat", type=str, default="37.5296", help="Ï§ëÏã¨ ÏúÑÎèÑ (Ïòà: 37.5296 for Ïö©ÏÇ∞Ïó≠)")
    parser.add_argument("--radius", type=int, default=2000, help="Í≤ÄÏÉâ Î∞òÍ≤Ω(ÎØ∏ÌÑ∞). Í∏∞Î≥∏Í∞í 2000 (2km)")
    parser.add_argument("--count", type=int, help="ÏµúÎåÄ ÌÅ¨Î°§ÎßÅÌï† ÏãùÎãπ Ïàò (ÏÑ§Ï†ï ÌååÏùºÏùò Í∞íÏùÑ ÎçÆÏñ¥ÏîÄ)")
    args = parser.parse_args()

    config = load_config()

    max_crawl_count = args.count if args.count is not None else config.max_crawl_count

    with KakaoMapCrawler(config) as crawler:
        crawler.run(lon=args.lon, lat=args.lat, radius=args.radius, max_count=max_crawl_count)

if __name__ == "__main__":
    main()
