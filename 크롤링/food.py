import requests
import json
import sys
import os
import time
import logging
import math
import argparse
import csv
import copy
from queue import Queue, Empty
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps

from bs4 import BeautifulSoup
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.remote.webdriver import WebDriver
from selenium.webdriver.remote.webelement import WebElement
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException

# --- 0. ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° ---

@dataclass(frozen=True)
class Config:
    """ìŠ¤í¬ë¦½íŠ¸ ì„¤ì • ê°’ì„ ê´€ë¦¬"""
    kakao_api_key: str
    chromedriver_path: str  # chromedriver ê²½ë¡œë¥¼ ì§ì ‘ ë°›ë„ë¡ ì„¤ì •
    max_workers: int
    category_map: Dict[str, List[str]]
    max_crawl_count: int = 10
    scroll_attempts: int = 10
    max_reviews_per_star: int = 3

def setup_logging():
    """ìŠ¤í¬ë¦½íŠ¸ ì „ì—­ ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

def load_config(path: str = "config.json") -> Config:
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œ"""
    if not os.path.exists(path):
        logging.critical(f"'{path}' ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì˜ˆì‹œ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ìƒì„±í•´ì£¼ì„¸ìš”.")
        sys.exit(1)
    with open(path, 'r', encoding='utf-8') as f:
        config_data = json.load(f)
    return Config(**config_data)

def retry(attempts: int = 3, delay: float = 1.0):
    """ì˜ˆì™¸ ë°œìƒ ì‹œ ì‘ì—…ì„ ì¬ì‹œë„í•˜ëŠ” ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    logging.warning(f"'{func.__name__}' ì‘ì—… ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{attempts}). {delay}ì´ˆ í›„ ì¬ì‹œë„. ì˜¤ë¥˜: {e}")
                    time.sleep(delay)
            raise last_exception
        return wrapper
    return decorator

# --- 1. í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ì •ì˜ ---

class KakaoMapCrawler:
    """ì¹´ì¹´ì˜¤ë§µ í¬ë¡¤ëŸ¬ (ì•ˆì •í™”ëœ ë“œë¼ì´ë²„ í’€ ë° íš¨ìœ¨ì ì¸ ìŠ¤í¬ë¡¤)"""

    DISCOVERY_API_URL = "https://dapi.kakao.com/v2/local/search/keyword.json"

    SELECTORS = {
        "rating": "span.num_star",
        "menu_tab": 'a.link_tab[href="#menuInfo"]',
        "menu_wrap": "div.wrap_goods",
        "menu_list": 'ul.list_goods > li',
        "menu_name": 'strong.tit_item',
        "menu_price": 'p.desc_item',
        "review_tab": 'a.link_tab[href="#comment"]',
        "review_list_wrap": "ul.list_review",
        "review_item": "ul.list_review > li",
        "review_rating": "span.figure_star.on",
        "review_content": "p.desc_review",
    }

    def __init__(self, config: Config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({"Authorization": f"KakaoAK {self.config.kakao_api_key}"})
        self.driver_pool = Queue(maxsize=self.config.max_workers)
        self._initialize_driver_pool()

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def _create_driver(self) -> WebDriver:
        """ìƒˆë¡œìš´ Selenium WebDriver ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±"""
        try:
            options = Options()
            options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})
            options.add_argument("--headless"); options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage"); options.add_argument("--log-level=3")
            # ì„¤ì • íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ë“œë¼ì´ë²„ ì„œë¹„ìŠ¤ ì„¤ì •
            service = Service(executable_path=self.config.chromedriver_path)
            driver = webdriver.Chrome(service=service, options=options)
            return driver
        except Exception as e:
            logging.error(f"ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _initialize_driver_pool(self):
        """ë“œë¼ì´ë²„ í’€ì„ ì´ˆê¸°í™”"""
        logging.info(f"Selenium ë“œë¼ì´ë²„ í’€ ì´ˆê¸°í™” ì¤‘... (ì›Œì»¤ ìˆ˜: {self.config.max_workers})")
        for _ in range(self.config.max_workers):
            try:
                driver = self._create_driver()
                self.driver_pool.put(driver)
            except Exception:
                continue
        if self.driver_pool.empty():
            logging.critical("ì‚¬ìš© ê°€ëŠ¥í•œ ë“œë¼ì´ë²„ê°€ ì—†ìŠµë‹ˆë‹¤. Chromedriver ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”. ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            sys.exit(1)
        logging.info(f"ì´ {self.driver_pool.qsize()}ê°œì˜ ë“œë¼ì´ë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def discover_restaurants(self, lon: str, lat: str, radius: int, target_count: int) -> List[Dict[str, Any]]:
        """ìœ„ì¹˜ ê¸°ë°˜ ë§›ì§‘ íƒìƒ‰"""
        center_lon, center_lat = float(lon), float(lat)
        logging.info(f"ğŸ—ºï¸  ì¢Œí‘œ (ê²½ë„:{lon}, ìœ„ë„:{lat}) ë°˜ê²½ {radius}m ë‚´ì—ì„œ ìµœëŒ€ {target_count}ê°œ ìŒì‹ì  ê²€ìƒ‰ ì‹œì‘...")

        unique_restaurants = {}
        lat_offset = (radius / 2) / 111000
        lon_offset = (radius / 2) / (111000 * math.cos(math.radians(center_lat)))
        search_points = [(str(center_lon + j * lon_offset), str(center_lat + i * lat_offset)) for i in range(-1, 2) for j in range(-1, 2)]

        for p_lon, p_lat in search_points:
            if len(unique_restaurants) >= target_count:
                logging.info(f"ëª©í‘œ ê°œìˆ˜ {target_count}ê°œë¥¼ ì´ˆê³¼í•˜ì—¬ íƒìƒ‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
            for page in range(1, 4):
                params = {"query": "ë§›ì§‘", "page": page, "size": 15, "category_group_code": "FD6",
                          'x': p_lon, 'y': p_lat, 'radius': radius, 'sort': 'distance'}
                try:
                    response = self.session.get(self.DISCOVERY_API_URL, params=params, timeout=5)
                    response.raise_for_status()
                    data = response.json()
                    if not data.get('documents'): break
                    for doc in data['documents']:
                        unique_restaurants[doc['id']] = doc
                    if len(unique_restaurants) >= target_count or data['meta']['is_end']: break
                except requests.RequestException as e:
                    logging.error(f"API ìš”ì²­ ì˜¤ë¥˜ (Lon: {p_lon}, Lat: {p_lat}, Page: {page}): {e}")
                    break

        restaurant_list = list(unique_restaurants.values())
        logging.info(f"âœ… ì´ {len(search_points)}ê°œ ì§€ì  íƒìƒ‰ ì™„ë£Œ. {len(restaurant_list)}ê°œì˜ ê³ ìœ  í¬ë¡¤ë§ ëŒ€ìƒ í™•ì •.")
        return restaurant_list

    def _scrape_rating(self, wait: WebDriverWait) -> Optional[str]:
        try:
            rating_tag = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["rating"])))
            return rating_tag.text
        except TimeoutException:
            return None

    def _scrape_menus(self, driver: WebDriver, wait: WebDriverWait) -> List[Dict[str, str]]:
        menus = []
        try:
            menu_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, self.SELECTORS["menu_tab"])))
            menu_tab.click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["menu_wrap"])))

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            for item in soup.select(self.SELECTORS["menu_list"]):
                name_tag = item.select_one(self.SELECTORS["menu_name"])
                if not name_tag or "ë”ë³´ê¸°" in name_tag.get_text(): continue

                price_tag = item.select_one(self.SELECTORS["menu_price"])
                menus.append({
                    "name": name_tag.get_text(strip=True),
                    "price": price_tag.get_text(strip=True) if price_tag else "ê°€ê²© ì •ë³´ ì—†ìŒ"
                })
        except TimeoutException:
            pass
        return menus

    def _scrape_reviews(self, driver: WebDriver, wait: WebDriverWait) -> Dict[int, List[str]]:
        """ë¦¬ë·° ìŠ¤í¬ë¡¤ ë° íŒŒì‹± íš¨ìœ¨ ê°œì„ """
        reviews = {star: [] for star in range(1, 6)}
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, self.SELECTORS["review_tab"])))
            review_tab.click()
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, self.SELECTORS["review_list_wrap"])))

            last_review_count = 0
            for _ in range(self.config.scroll_attempts):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(0.8)

                current_reviews = driver.find_elements(By.CSS_SELECTOR, self.SELECTORS["review_item"])
                if len(current_reviews) == last_review_count:
                    break
                last_review_count = len(current_reviews)

            all_review_elements = driver.find_elements(By.CSS_SELECTOR, self.SELECTORS["review_item"])
            for review_item_el in all_review_elements:
                soup = BeautifulSoup(review_item_el.get_attribute('outerHTML'), 'html.parser')
                rating = len(soup.select(self.SELECTORS["review_rating"]))
                content_tag = soup.select_one(self.SELECTORS["review_content"])
                content = content_tag.get_text(strip=True) if content_tag else ""

                if rating in reviews and len(reviews[rating]) < self.config.max_reviews_per_star:
                    reviews[rating].append(content)
        except TimeoutException:
            pass
        return reviews

    def _scrape_page_data(self, driver: WebDriver, place_id: str) -> Dict[str, Any]:
        base_url = f"https://place.map.kakao.com/{place_id}"
        data = {"overall_rating": None, "menus": [], "reviews": {}}

        driver.get(base_url)
        wait = WebDriverWait(driver, 10)

        data["overall_rating"] = self._scrape_rating(wait)
        data["menus"] = self._scrape_menus(driver, wait)
        data["reviews"] = self._scrape_reviews(driver, wait)

        return data

    def _simplify_category(self, category_name: Optional[str]) -> str:
        """ì„¤ì • íŒŒì¼ì˜ ì¹´í…Œê³ ë¦¬ ë§µì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜"""
        if not category_name:
            return "ê¸°íƒ€"
        for simple_cat, keywords in self.config.category_map.items():
            if any(keyword in category_name for keyword in keywords):
                return simple_cat
        return "ê¸°íƒ€"

    @retry(attempts=2, delay=2)
    def _process_restaurant(self, restaurant_info: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ë ˆìŠ¤í† ë‘ ì²˜ë¦¬ (ì•ˆì •ì ì¸ ë“œë¼ì´ë²„ êµì²´ ë¡œì§ í¬í•¨)"""
        place_name, place_id = restaurant_info.get('place_name'), restaurant_info.get('id')
        if not all([place_name, place_id]): return None

        driver = None
        driver_is_faulty = False
        try:
            driver = self.driver_pool.get(timeout=20)
            scraped_data = self._scrape_page_data(driver, place_id)

            detailed_category = restaurant_info.get('category_name')
            simple_category = self._simplify_category(detailed_category)

            return {
                "restaurant_name": place_name, "category": simple_category,
                "overall_rating": scraped_data["overall_rating"],
                "phone_number": restaurant_info.get('phone'),
                "address": restaurant_info.get('road_address_name'),
                "latitude": restaurant_info.get('y'), "longitude": restaurant_info.get('x'),
                "kakao_map_url": restaurant_info.get('place_url'),
                "menu_items": scraped_data["menus"], "reviews_by_star": scraped_data["reviews"],
                "crawled_at": datetime.now().isoformat()
            }
        except WebDriverException as e:
            driver_is_faulty = True
            logging.error(f"'{place_name}' ì²˜ë¦¬ ì¤‘ ë“œë¼ì´ë²„ ì˜¤ë¥˜ ë°œìƒ: {e.msg[:100]}")
            raise e
        except Empty:
            logging.error("ë“œë¼ì´ë²„ í’€ì´ ë¹„ì–´ìˆì–´ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            logging.error(f"'{place_name}'({place_id}) ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=False)
            raise e
        finally:
            if driver:
                if driver_is_faulty:
                    logging.warning(f"ê²°í•¨ì´ ë°œìƒí•œ ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•˜ê³  ìƒˆ ë“œë¼ì´ë²„ë¡œ êµì²´í•©ë‹ˆë‹¤.")
                    try:
                        driver.quit()
                        new_driver = self._create_driver()
                        self.driver_pool.put(new_driver)
                    except Exception as create_err:
                        logging.error(f"ìƒˆ ë“œë¼ì´ë²„ ìƒì„±/ì¶”ê°€ ì‹¤íŒ¨: {create_err}")
                else:
                    self.driver_pool.put(driver)

    def run(self, lon: str, lat: str, radius: int, max_count: int):
        restaurant_list = self.discover_restaurants(lon, lat, radius, target_count=max_count)
        if not restaurant_list: return

        restaurant_list_to_crawl = restaurant_list[:max_count]
        logging.info(f"íƒìƒ‰ëœ {len(restaurant_list)}ê°œ ì¤‘, {len(restaurant_list_to_crawl)}ê°œì— ëŒ€í•œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

        all_results = []
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            future_to_info = {executor.submit(self._process_restaurant, info): info for info in restaurant_list_to_crawl}
            for future in tqdm(as_completed(future_to_info), total=len(restaurant_list_to_crawl), desc="í¬ë¡¤ë§ ì§„í–‰ë¥ "):
                try:
                    result = future.result()
                    if result: all_results.append(result)
                except Exception as e:
                    info = future_to_info[future]
                    logging.error(f"'{info.get('place_name')}' ìµœì¢… ì²˜ë¦¬ ì‹¤íŒ¨: {e.__class__.__name__}")

        if all_results:
            self.save_results(all_results)
        else:
            logging.warning("ìœ íš¨í•œ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logging.info("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def save_results(self, results: List[Dict[str, Any]]):
        output_dir = "json_output"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        base_filename = os.path.join(output_dir, f"search_results_{timestamp}")

        # JSON ì €ì¥
        json_filename = f"{base_filename}.json"
        with open(json_filename, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        logging.info(f"âœ… ì„±ê³µ! {len(results)}ê°œ ìŒì‹ì  ì •ë³´ë¥¼ '{json_filename}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

        # CSV ì €ì¥
        if not results: return
        csv_filename = f"{base_filename}.csv"
        results_for_csv = copy.deepcopy(results)
        for item in results_for_csv:
            item['menu_items'] = json.dumps(item['menu_items'], ensure_ascii=False)
            item['reviews_by_star'] = json.dumps(item['reviews_by_star'], ensure_ascii=False)

        fieldnames = results_for_csv[0].keys()
        with open(csv_filename, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results_for_csv)
        logging.info(f"âœ… ì„±ê³µ! ë™ì¼í•œ ì •ë³´ë¥¼ '{csv_filename}'ì—ë„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    def close(self):
        logging.info("ëª¨ë“  Selenium ë“œë¼ì´ë²„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
        while not self.driver_pool.empty():
            try:
                driver = self.driver_pool.get_nowait()
                driver.quit()
            except Empty: break
        self.session.close()
        logging.info("ì •ë¦¬ ì™„ë£Œ.")

# --- 2. ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
def main():
    setup_logging()

    parser = argparse.ArgumentParser(description="ì¹´ì¹´ì˜¤ë§µ ë§›ì§‘ ì •ë³´ í¬ë¡¤ëŸ¬ (ìˆ˜ë™ ê²½ë¡œ ì„¤ì •)")
    parser.add_argument("--lon", type=str, default="126.9648", help="ì¤‘ì‹¬ ê²½ë„ (ì˜ˆ: 126.9648 for ìš©ì‚°ì—­)")
    parser.add_argument("--lat", type=str, default="37.5296", help="ì¤‘ì‹¬ ìœ„ë„ (ì˜ˆ: 37.5296 for ìš©ì‚°ì—­)")
    parser.add_argument("--radius", type=int, default=2000, help="ê²€ìƒ‰ ë°˜ê²½(ë¯¸í„°). ê¸°ë³¸ê°’ 2000 (2km)")
    parser.add_argument("--count", type=int, help="ìµœëŒ€ í¬ë¡¤ë§í•  ì‹ë‹¹ ìˆ˜ (ì„¤ì • íŒŒì¼ì˜ ê°’ì„ ë®ì–´ì”€)")
    args = parser.parse_args()

    config = load_config()

    max_crawl_count = args.count if args.count is not None else config.max_crawl_count

    with KakaoMapCrawler(config) as crawler:
        crawler.run(lon=args.lon, lat=args.lat, radius=args.radius, max_count=max_crawl_count)

if __name__ == "__main__":
    main()
